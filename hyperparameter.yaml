---
num_blocks: [2, 4]
patch_size: [4, 8]
hidden_dim: [8, 16, 32]
tokens_mlp_dim: [8, 16, 32]
# channels_mlp_dim: [16, 32, 64]  # To reduce time, we'll assume channels_mlp_dim = tokens_mlp_dim
batch_size: [8, 16, 32]
epochs: [20]  # To reduce time, we'll use epochs = 20 as default
# optimizer: ['SGD']  # To reduce time, let's stick to SGD
init_lr: [0.075]  # To reduce time.......
